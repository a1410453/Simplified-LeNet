{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbz1S4iG1s1z"
      },
      "source": [
        "Step 1: Go to https://colab.research.google.com in Browser and Click on New Python 3 Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZnuIG1x121J"
      },
      "source": [
        "Step 2: Click to Runtime > Change runtime type > Hardware Accelerator, choose GPU ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeGoZQ0F13EN"
      },
      "source": [
        "Step 3: Check the Version of CUDA by : running the command below to get the following output (if not available, install cuda-10.1: \n",
        "!apt-get update\n",
        "!apt-get install cuda-10.1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmFf7jza1ixU",
        "outputId": "a0ce3770-78ae-439d-94fa-dc894f9d6c25"
      },
      "source": [
        "#!apt-get update \n",
        "#!apt-get install cuda-10.1\n",
        "!nvcc --version"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdWxYqPoAald",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35f2e079-3ddc-462a-90d6-fc8d527ada65"
      },
      "source": [
        "#!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-r1v076zl\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-r1v076zl\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit aac710a35f52bb78ab34d2e52517237941399eff\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4305 sha256=944af23c14266483ea07f43469d3e7b6196d856c8223e25b31ca665a21433927\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-t7rutq86/wheels/db/c1/1f/a2bb07bbb4a1ce3c43921252aeafaa6205f08637e292496f04\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n",
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8cFr08dtLAH"
      },
      "source": [
        "## Simulation test\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "code1 = \"\"\"\n",
        "/* CPU version */ \n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "\n",
        "#define INSIZE 28\n",
        "#define NUM_TEST_IMAGES 10000\n",
        "#define X1 6 // Number of blocks for kernel_conv_filter\n",
        "#define Y1 24 // Number of threads per block for kernel_conv_filter\n",
        "#define X2 6 // Number of blocks for kernel_conv_bias\n",
        "#define Y2 24 // Number of threads per block for kernel_conv_bias\n",
        "#define X3 6 // Number of blocks for kernel_conv_sigmoid\n",
        "#define Y3 24 // Number of threads per block for kernel_conv_sigmoid\n",
        "\n",
        "typedef struct mnist_data{\n",
        "    double data[INSIZE][INSIZE];\n",
        "    unsigned int label;\n",
        "} mnist_data;\n",
        "\n",
        "\n",
        "static unsigned int mnist_bin_to_int(char *tmp) {\n",
        "    unsigned int val = 0;\n",
        "    for (int i = 0; i < 4; i++) {\n",
        "        val <<= 8; // bit shift\n",
        "        val |= (unsigned char)tmp[i];// bitwise or\n",
        "    }\n",
        "    return val;\n",
        "}\n",
        "\n",
        "\n",
        "static int mnist_load(const char *image_filename, const char *label_filename, mnist_data **data_set, unsigned int *count) {\n",
        "    // 1. opens image and label files of the test\n",
        "    FILE *image_file = fopen(image_filename, \"rb\");\n",
        "    FILE *label_file = fopen(label_filename, \"rb\");\n",
        "    if (label_file == NULL || image_file == NULL) {\n",
        "        printf(\"Failed to open image file or label file\\\\n\");\n",
        "        return -1;\n",
        "    }\n",
        "\n",
        "    // 2. check the file formats of the test files\n",
        "\n",
        "    // 2-1. the magic numbers of image and label files\n",
        "    char image_magic_number[4]; \n",
        "    char label_magic_number[4];\n",
        "    fread(&image_magic_number, sizeof(char), 4, image_file);\n",
        "    fread(&label_magic_number, sizeof(char), 4, label_file);\n",
        "    if (mnist_bin_to_int(image_magic_number) != 2051 || mnist_bin_to_int(label_magic_number) != 2049) {\n",
        "        printf(\"Invalid magic numbers in files\\\\n\");\n",
        "        fclose(image_file);\n",
        "        fclose(label_file);\n",
        "        return -1;\n",
        "    }else{\n",
        "      printf(\"image magic number= 2051\\\\n\");\n",
        "      printf(\"label magic number = 2049\\\\n\");\n",
        "    }\n",
        "\n",
        "\n",
        "    // 2-2. numbers of images and labels\n",
        "    char num_images[4];\n",
        "    char num_labels[4];\n",
        "    fread(&num_images, sizeof(char), 4, image_file);\n",
        "    fread(&num_labels, sizeof(char), 4, label_file);\n",
        "\n",
        "    if (mnist_bin_to_int(num_images) != NUM_TEST_IMAGES || mnist_bin_to_int(num_labels) != NUM_TEST_IMAGES) {\n",
        "        printf(\"Invalid number of images/labels in files\\\\n\");\n",
        "        fclose(image_file);\n",
        "        fclose(label_file);\n",
        "        return -1;\n",
        "    }else{\n",
        "      printf(\"image total number = 10000\\\\n\");\n",
        "      printf(\"label total number = 10000\\\\n\");\n",
        "    }\n",
        "\n",
        "    // 2-4. check the number of rows and columns\n",
        "    // Check the number of rows and columns\n",
        "    char num_rows[4];\n",
        "    char num_cols[4];\n",
        "    fread(&num_rows, sizeof(char), 4, image_file);\n",
        "    fread(&num_cols, sizeof(char), 4, image_file);\n",
        "    if (mnist_bin_to_int(num_rows) != INSIZE || mnist_bin_to_int(num_cols) != INSIZE) {\n",
        "        printf(\"Invalid image size\\\\n\");\n",
        "        fclose(image_file);\n",
        "        fclose(label_file);\n",
        "        return -1;\n",
        "    }else{\n",
        "      printf(\"rows = 28, cols = 28\\\\n\");\n",
        "    }\n",
        "\n",
        "    // Allocate memory for the data set\n",
        "    mnist_data *data = (mnist_data*)malloc(NUM_TEST_IMAGES * sizeof(mnist_data));\n",
        "\n",
        "    int counter = 0;\n",
        "\n",
        "    // 3. loads image data as double type (from 0.0 to 1.0 dividing unsigned char values by 255.0) \n",
        "    unsigned char image[INSIZE][INSIZE];\n",
        "    for (int i = 0; i < NUM_TEST_IMAGES; i++) {\n",
        "        fread(image, sizeof(image), 1, image_file);\n",
        "        data[i].label = fgetc(label_file);\n",
        "        counter++;\n",
        "        for (int j = 0; j < INSIZE; j++) {\n",
        "            for (int k = 0; k < INSIZE; k++) {\n",
        "                data[i].data[j][k] = (double)image[j][k] / 255.0;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // 4. closes opened files\n",
        "    fclose(image_file);\n",
        "    fclose(label_file);\n",
        "\n",
        "    // Set the output variables\n",
        "    *data_set = data;\n",
        "    *count = counter;\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "    // CUDA kernel functions for filtering, bias, and sigmoid activation\n",
        "\n",
        "    __global__ void kernel_conv_filter(float (*input)[28], float (*pre_output)[24][24], float (*weight)[5][5]) {\n",
        "            int t = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "            int i = t / (24 * 24);\n",
        "            int j = (t / 24) % 24;\n",
        "            int k = t % 24;\n",
        "\n",
        "            if (t < 6 * 24 * 24) {\n",
        "                float sum = 0.0f;\n",
        "                for (int m = 0; m < 5; m++) {\n",
        "                    for (int n = 0; n < 5; n++) {\n",
        "                        sum += weight[i][m][n] * input[j + m][k + n];\n",
        "                    }\n",
        "                }\n",
        "                pre_output[i][j][k] = sum;\n",
        "            }\n",
        "    }\n",
        "\n",
        "\n",
        "    __global__ void kernel_conv_bias(float (*pre_output)[24][24], float *bias) {\n",
        "        int t = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "        int i = t / (24 * 24);\n",
        "        int j = (t / 24) % 24;\n",
        "        int k = t % 24;\n",
        "\n",
        "        if (t < 6 * 24 * 24) {\n",
        "            pre_output[i][j][k] += bias[i];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    __global__ void kernel_conv_sigmoid(float pre_output[6][24][24], float output[6][24][24]) {\n",
        "        int t = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "        int i = t / (24 * 24);\n",
        "        int j = (t / 24) % 24;\n",
        "        int k = t % 24;\n",
        "        if (t < 6 * 24 * 24) {\n",
        "           output[i][j][k] = 1.0f / (1.0f + expf(-pre_output[i][j][k]));\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    __global__ void kernel_ss1_filter(float input[6][24][24], float output[6][6][6], float weight[4][4]) {\n",
        "        int row = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "        int col = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "        int feature = blockIdx.z;\n",
        "        \n",
        "        if (row < 6 && col < 6) {\n",
        "            float sum = 0.0;\n",
        "            for (int i = 0; i < 4; i++) {\n",
        "                for (int j = 0; j < 4; j++) {\n",
        "                    sum += input[feature][row*4+i][col*4+j] * weight[i][j];\n",
        "                }\n",
        "            }\n",
        "            output[feature][row][col] = sum;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    __global__ void kernel_ss1_bias(float output[6][6][6]) {\n",
        "        int row = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "        int col = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "        int feature = blockIdx.z;\n",
        "        if (row < 6 && col < 6) {\n",
        "            output[feature][row][col] = output[feature][row][col] - 1;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    __global__ void kernel_ss1_sigmoid(float output[6][6][6]) {\n",
        "        int row = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "        int col = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "        int feature = blockIdx.z;\n",
        "        if (row < 6 && col < 6) {\n",
        "          output[feature][row][col] = 1.0f / (1.0f + expf(-output[feature][row][col]));\n",
        "        }\n",
        "    }\n",
        "\n",
        "__global__ void kernel_fc1(float input[6][6][6], float pre_output[10], float weight[10][6][6][6]) {\n",
        "    // Calculate global thread index\n",
        "    int t = threadIdx.x + blockIdx.x * blockDim.x ;\n",
        "    if(t<10){\n",
        "          // Compute dot product of input and weight for current output index\n",
        "    float dot_product = 0.0f;\n",
        "    for (int x = 0; x < 6; x++) {\n",
        "        for (int y = 0; y < 6; y++) {\n",
        "            for (int z = 0; z < 6; z++) {\n",
        "                dot_product += input[x][y][z] * weight[t][x][y][z];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    // Store dot product in pre_output array\n",
        "    pre_output[t] = dot_product;\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void kernel_fc1_bias(float pre_output[10], float bias[10]) {\n",
        "    int t = threadIdx.x +  blockIdx.x * blockDim.x;\n",
        "    if (t < 10) {\n",
        "        pre_output[t] += bias[t];\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void kernel_fc1_sigmoid(float pre_output[10], float output[10]) {\n",
        "    int t = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if (t < 10) {\n",
        "        output[t] = 1.0f / (1.0f + expf(-pre_output[t]));\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "class Layer {\n",
        "public:\n",
        "    int M, N, O;\n",
        "    float pre_output[6][24][24], output[6][24][24];\n",
        "    float weight[6][5][5], bias[24];\n",
        "\n",
        "    float ssweight[4][4];\n",
        "    float ssoutput[6][6][6];\n",
        "\n",
        "    float (*dpre_output)[24][24], (*doutput)[24][24];\n",
        "    float (*dweight)[5][5], (*dbias);\n",
        "    float (*dssweight)[4], (*dssoutput)[6][6];\n",
        "\n",
        "    Layer(int M, int N, int O) {\n",
        "        this->M = M;\n",
        "        this->N = N;\n",
        "        this->O = O;\n",
        "\n",
        "        // Allocate memory on the GPU\n",
        "        cudaMalloc(&dpre_output, 6 * O * O * sizeof(float));\n",
        "        cudaMalloc(&doutput, 6 * O * O * sizeof(float));\n",
        "        cudaMalloc(&dweight, M * 5 * 5 * sizeof(float));\n",
        "        cudaMalloc(&dbias, O * sizeof(float));\n",
        "        cudaMalloc(&dssweight, 4 * 4 *sizeof(float));\n",
        "        cudaMalloc(&dssoutput, 6 * 6 * 6 * sizeof(float));\n",
        "\n",
        "      for(int i=0; i<6; i++){\n",
        "        for(int j=0; j<5; j++){\n",
        "          for(int k=0; k<5; k++){\n",
        "            weight[i][j][k] = -1.0;\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "\n",
        "      for(int i=0; i<6; i++){\n",
        "            bias[i] = -1.0;\n",
        "      }\n",
        "\n",
        "      for(int j=0; j<4; j++){\n",
        "        for(int k=0; k<4; k++){\n",
        "          ssweight[j][k] = -1.0;\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    ~Layer() {\n",
        "        // Free memory on the GPU\n",
        "        cudaFree(dpre_output);\n",
        "        cudaFree(doutput);\n",
        "        cudaFree(dweight);\n",
        "        cudaFree(dbias);\n",
        "    }\n",
        "};\n",
        "\n",
        "void forward_pass(double data[28][28],Layer layer) {\n",
        "        float input[28][28];\n",
        "        // Convert the input data to -1 to simulate test\n",
        "        for (int i = 0; i < 28; i++) {\n",
        "            for (int j = 0; j < 28; j++) {\n",
        "                input[i][j] = -1.0;\n",
        "            }\n",
        "        }\n",
        "\n",
        "\n",
        "        //CONVOLUTION\n",
        "\n",
        "        float (*dinput)[28];\n",
        "        cudaMalloc(&dinput, 28 * 28 * sizeof(float));\n",
        "        cudaMemcpy(dinput, input, 28 * 28 * sizeof(float), cudaMemcpyHostToDevice);\n",
        "        cudaMemcpy(layer.dweight, layer.weight, 6 * 5 * 5 * sizeof(float), cudaMemcpyHostToDevice);\n",
        "        cudaMemcpy(layer.dbias, layer.bias, 6 * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "        cudaMemcpy(layer.dssweight, layer.ssweight, 4 * 4 * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "        printf(\"\\\\n\");\n",
        "        printf(\"Convolution\\\\n\");\n",
        "        // Perform the filtering operation\n",
        "        kernel_conv_filter<<<6, 576>>>(dinput, layer.dpre_output, layer.dweight);\n",
        "        float sum = 0;\n",
        "        float maxerror;      \n",
        "        cudaMemcpy(layer.output, layer.dpre_output, 6 * 24 * 24 * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "        sum = 0;\n",
        "        for (int i = 0; i < 6; i++) {\n",
        "            for (int j = 0; j < 24; j++) {\n",
        "                for (int k = 0; k < 24; k++) {\n",
        "                    \n",
        "                    if (layer.output[i][j][k] != 25.0){\n",
        "                      sum += 1.0;\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        maxerror = max(0.0, sum);\n",
        "        printf(\"After Filtering:\\\\npreact maxError = %f (asserted with 25.0)\\\\n\", maxerror);\n",
        "        \n",
        "        \n",
        "        // Add the bias term\n",
        "        kernel_conv_bias<<<6, 576>>>(layer.dpre_output, layer.dbias);\n",
        "        cudaMemcpy(layer.output, layer.dpre_output, 6 * 24 * 24 * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "        sum=0;\n",
        "        for (int i = 0; i < 6; i++) {\n",
        "            for (int j = 0; j < 24; j++) {\n",
        "                for (int k = 0; k < 24; k++) {\n",
        "                    if (layer.output[i][j][k] != 24.0){\n",
        "                      sum += 1.0;\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        maxerror = max(0.0, sum);\n",
        "        printf(\"After Bias:\\\\npreact maxError = %f (asserted with 24.0)\\\\n\", maxerror);\n",
        "        \n",
        "        // Apply the sigmoid activation function\n",
        "        kernel_conv_sigmoid<<<6, 576>>>(layer.dpre_output, layer.doutput);\n",
        "        cudaMemcpy(layer.output, layer.doutput, 6 * 24 * 24 * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "        sum=0;\n",
        "        for (int i = 0; i < 6; i++) {\n",
        "            for (int j = 0; j < 24; j++) {\n",
        "                for (int k = 0; k < 24; k++) {\n",
        "                    \n",
        "                    if (layer.output[i][j][k] != 1.0){\n",
        "                      sum += 1.0;\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        maxerror = max(0.0, sum);\n",
        "        printf(\"After Sigmoid:\\\\npreact maxError = %f (asserted with 1.0)\\\\n\", maxerror);\n",
        "        \n",
        "\n",
        "        //SUBSAMPLING\n",
        "\n",
        "        dim3 blockDim(16, 16);\n",
        "        dim3 gridDim(2, 2, 6);\n",
        "\n",
        "\n",
        "\n",
        "        printf(\"\\\\n\");\n",
        "        printf(\"Subsampling\\\\n\");\n",
        "\n",
        "        kernel_ss1_filter<<<gridDim, blockDim>>>(layer.doutput, layer.dssoutput, layer.dssweight);\n",
        "        cudaMemcpy(layer.ssoutput, layer.dssoutput, 6 * 6 * 6 * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "        sum=0;\n",
        "        for (int i = 0; i < 6; i++) {\n",
        "            for (int j = 0; j < 6; j++) {\n",
        "                for (int k = 0; k < 6; k++) {\n",
        "                    if (layer.ssoutput[i][j][k] != -16.0){\n",
        "                      sum += 1.0;\n",
        "                    }\n",
        "                }   \n",
        "            }\n",
        "        }\n",
        "        maxerror = max(0.0, sum);\n",
        "        printf(\"After Filtering:\\\\npreact maxError = %f (asserted with -16.0)\\\\n\", maxerror);\n",
        "        \n",
        "\n",
        "\n",
        "        kernel_ss1_bias<<<gridDim, blockDim>>>(layer.dssoutput);\n",
        "        cudaMemcpy(layer.ssoutput, layer.dssoutput, 6 * 6 * 6 * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "        sum=0;\n",
        "        for (int i = 0; i < 6; i++) {\n",
        "            for (int j = 0; j < 6; j++) {\n",
        "                for (int k = 0; k < 6; k++) {\n",
        "                    if (layer.ssoutput[i][j][k] != -17.0){\n",
        "                      sum += 1.0;\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        maxerror = max(0.0, sum);\n",
        "        printf(\"After Bias:\\\\npreact maxError = %f (asserted with -17.0)\\\\n\", maxerror);\n",
        "\n",
        "        kernel_ss1_sigmoid<<<gridDim, blockDim>>>(layer.dssoutput);\n",
        "        cudaMemcpy(layer.ssoutput, layer.dssoutput, 6 * 6 * 6 * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "        sum=0;\n",
        "        for (int i = 0; i < 6; i++) {\n",
        "            for (int j = 0; j < 6; j++) {\n",
        "                for (int k = 0; k < 6; k++) {\n",
        "                    \n",
        "                    if (layer.ssoutput[i][j][k] == 0){\n",
        "                      sum += 1.0;\n",
        "                      \n",
        "                    }\n",
        "      \n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        maxerror = max(0.0, sum);\n",
        "        printf(\"After Sigmoid:\\\\npreact maxError = %f (asserted with 0.0)\\\\n\", maxerror);\n",
        "        printf(\"\\\\n\");\n",
        "\n",
        "        \n",
        "\n",
        "        //MARK: changing input for fc to test fc\n",
        "        for (int i = 0; i < 6; i++) {\n",
        "            for (int j = 0; j < 6; j++) {\n",
        "                for (int k = 0; k < 6; k++) {  \n",
        "                  layer.ssoutput[i][j][k] = 1;\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        \n",
        "\n",
        "\n",
        "        cudaMemcpy(layer.dssoutput, layer.ssoutput, 6 * 6 * 6 * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "        printf(\"FC\\\\n\");\n",
        "\n",
        "        float fc_output[10];\n",
        "        float fc_weight[10][6][6][6]; \n",
        "        float fc_bias[10];\n",
        "\n",
        "        for (int i = 0; i < 10; i++) {\n",
        "            fc_bias[i]=1.0;\n",
        "            for (int j = 0; j < 6; j++) {\n",
        "                for (int k = 0; k < 6; k++) {\n",
        "                    for (int x = 0; x < 6; x++) {\n",
        "                        fc_weight[i][j][k][x]=1.0;\n",
        "                    }\n",
        "                }   \n",
        "            }        \n",
        "        }\n",
        "\n",
        "        float (*dfc_pre_output);\n",
        "        float (*dfc_output);\n",
        "        float (*dfc_weight)[6][6][6]; \n",
        "        float (*dfc_bias);\n",
        "\n",
        "        cudaMalloc(&dfc_pre_output, 10 * sizeof(float));\n",
        "        cudaMalloc(&dfc_output, 10 * sizeof(float));\n",
        "        cudaMalloc(&dfc_weight, 10 * 6 * 6 * 6 * sizeof(float));\n",
        "        cudaMalloc(&dfc_bias, 10 * sizeof(float));\n",
        "\n",
        "        cudaMemcpy(dfc_weight, fc_weight, 10 * 6 * 6 * 6 * sizeof(float), cudaMemcpyHostToDevice);\n",
        "        cudaMemcpy(dfc_bias, fc_bias, 10 * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "        // Set grid and block sizes\n",
        "        //dim3 gridSize(10, 1, 1);\n",
        "        //dim3 blockSize(216, 1, 1);\n",
        "\n",
        "        // Call kernel_fc1 function\n",
        "        kernel_fc1<<<1, 10>>>(layer.dssoutput, dfc_pre_output, dfc_weight);\n",
        "    \n",
        "        cudaMemcpy(fc_output, dfc_pre_output, 10 * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "        sum=0;\n",
        "        for (int k = 0; k < 10; k++) {\n",
        "          if(fc_output[k] != 216){\n",
        "              sum+=1;\n",
        "          }\n",
        "        }\n",
        "\n",
        "        maxerror = max(0.0, sum);\n",
        "        \n",
        "        printf(\"After filter:\\\\npreact maxError = %f (asserted with 216)\\\\n\", maxerror);\n",
        "        \n",
        "        \n",
        "      \n",
        "        kernel_fc1_bias<<<1, 10>>>(dfc_pre_output, dfc_bias);\n",
        "      \n",
        "        cudaMemcpy(fc_output, dfc_pre_output, 10 * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "        sum=0;\n",
        "        for (int k = 0; k < 10; k++) {\n",
        "            if(fc_output[k] != 217){\n",
        "                sum+=1;\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        maxerror = max(0.0, sum);\n",
        "        printf(\"After bias:\\\\npreact maxError = %f (asserted with 217)\\\\n\", maxerror);\n",
        "\n",
        "\n",
        "        kernel_fc1_sigmoid<<<1, 10>>>(dfc_pre_output, dfc_output);\n",
        "\n",
        "        cudaMemcpy(fc_output, dfc_output, 10 * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "        sum=0;\n",
        "        for (int k = 0; k < 10; k++) {\n",
        "              if(fc_output[k] != 1){\n",
        "                sum+=1;\n",
        "          }\n",
        "        }\n",
        "        \n",
        "        maxerror = max(0.0, sum);\n",
        "        printf(\"After sigmoid:\\\\npreact maxError = %f (asserted with 1.0)\\\\n\", maxerror);\n",
        "        printf(\"\\\\n\");\n",
        "      \n",
        "}   \n",
        "\n",
        "\n",
        "\n",
        "int main() {\n",
        "    const int M = 6;\n",
        "    const int N = 1;\n",
        "    const int O = 24;\n",
        "\n",
        "\n",
        "    int ret, i; \n",
        "    mnist_data *test_set; \n",
        "    static unsigned int test_cnt;\n",
        "\n",
        "    // Initialize convolutional layer\n",
        "    Layer layer(M, N, O);\n",
        "    \n",
        "    // 1. load data\n",
        "    if(ret = mnist_load(\"data/t10k-images.idx3-ubyte\", \"data/t10k-labels.idx1-ubyte\", &test_set, &test_cnt) != 0)\n",
        "    printf(\"An error occured: %d \\\\n\", ret);\n",
        "   \n",
        "    // 2. forward pass\n",
        "    // for the first image \n",
        "    for (i=0; i<1; i++){ \n",
        "        forward_pass(test_set[i].data, layer);\n",
        "    }\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "/*START_CITE\n",
        "https://chat.openai.com/chat\n",
        "END_CITE*/\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "kpwLRMD1pGvM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = open(\"simulation.cu\", \"w\")\n",
        "text_file.write(code1)\n",
        "text_file.close()"
      ],
      "metadata": {
        "id": "NP0uYPGdrlxa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc simulation.cu -o simulation"
      ],
      "metadata": {
        "id": "Lrqhb7wvrmZN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./simulation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWiUkQzgrrL9",
        "outputId": "81c066ae-4b41-414d-8d17-9eaf3d8bb197"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to open image file or label file\n",
            "An error occured: 1 \n",
            "\n",
            "Convolution\n",
            "After Filtering:\n",
            "preact maxError = 0.000000 (asserted with 25.0)\n",
            "After Bias:\n",
            "preact maxError = 0.000000 (asserted with 24.0)\n",
            "After Sigmoid:\n",
            "preact maxError = 0.000000 (asserted with 1.0)\n",
            "\n",
            "Subsampling\n",
            "After Filtering:\n",
            "preact maxError = 0.000000 (asserted with -16.0)\n",
            "After Bias:\n",
            "preact maxError = 0.000000 (asserted with -17.0)\n",
            "After Sigmoid:\n",
            "preact maxError = 0.000000 (asserted with 0.0)\n",
            "\n",
            "FC\n",
            "After filter:\n",
            "preact maxError = 0.000000 (asserted with 216)\n",
            "After bias:\n",
            "preact maxError = 0.000000 (asserted with 217)\n",
            "After sigmoid:\n",
            "preact maxError = 0.000000 (asserted with 1.0)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mnist data test\n"
      ],
      "metadata": {
        "id": "mrts518cBtI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "code1 = \"\"\"\n",
        "\n",
        "\n",
        "#include \"slenet_params.h\"\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <string.h>\n",
        "\n",
        "#define INSIZE 28\n",
        "#define NUM_TEST_IMAGES 10000\n",
        "#define X1 6 // Number of blocks for kernel_conv_filter\n",
        "#define Y1 24 // Number of threads per block for kernel_conv_filter\n",
        "#define X2 6 // Number of blocks for kernel_conv_bias\n",
        "#define Y2 24 // Number of threads per block for kernel_conv_bias\n",
        "#define X3 6 // Number of blocks for kernel_conv_sigmoid\n",
        "#define Y3 24 // Number of threads per block for kernel_conv_sigmoid\n",
        "\n",
        "typedef struct mnist_data{\n",
        "    double data[INSIZE][INSIZE];\n",
        "    unsigned int label;\n",
        "} mnist_data;\n",
        "\n",
        "\n",
        "\n",
        "static unsigned int mnist_bin_to_int(char *tmp) {\n",
        "    unsigned int val = 0;\n",
        "    for (int i = 0; i < 4; i++) {\n",
        "        val <<= 8; // bit shift\n",
        "        val |= (unsigned char)tmp[i];// bitwise or\n",
        "    }\n",
        "    return val;\n",
        "}\n",
        "\n",
        "\n",
        "static int mnist_load(const char *image_filename, const char *label_filename, mnist_data **data_set, unsigned int *count) {\n",
        "    // 1. opens image and label files of the test\n",
        "    FILE *image_file = fopen(image_filename, \"rb\");\n",
        "    FILE *label_file = fopen(label_filename, \"rb\");\n",
        "    if (label_file == NULL || image_file == NULL) {\n",
        "        printf(\"Failed to open image file or label file\\\\n\");\n",
        "        return -1;\n",
        "    }\n",
        "\n",
        "    // 2. check the file formats of the test files\n",
        "\n",
        "    // 2-1. the magic numbers of image and label files\n",
        "    char image_magic_number[4]; \n",
        "    char label_magic_number[4];\n",
        "    fread(&image_magic_number, sizeof(char), 4, image_file);\n",
        "    fread(&label_magic_number, sizeof(char), 4, label_file);\n",
        "    if (mnist_bin_to_int(image_magic_number) != 2051 || mnist_bin_to_int(label_magic_number) != 2049) {\n",
        "        printf(\"Invalid magic numbers in files\\\\n\");\n",
        "        fclose(image_file);\n",
        "        fclose(label_file);\n",
        "        return -1;\n",
        "    }else{\n",
        "      printf(\"image magic number= 2051\\\\n\");\n",
        "      printf(\"label magic number = 2049\\\\n\");\n",
        "    }\n",
        "\n",
        "\n",
        "    // 2-2. numbers of images and labels\n",
        "    char num_images[4];\n",
        "    char num_labels[4];\n",
        "    fread(&num_images, sizeof(char), 4, image_file);\n",
        "    fread(&num_labels, sizeof(char), 4, label_file);\n",
        "\n",
        "    if (mnist_bin_to_int(num_images) != NUM_TEST_IMAGES || mnist_bin_to_int(num_labels) != NUM_TEST_IMAGES) {\n",
        "        printf(\"Invalid number of images/labels in files\\\\n\");\n",
        "        fclose(image_file);\n",
        "        fclose(label_file);\n",
        "        return -1;\n",
        "    }else{\n",
        "      printf(\"image total number = 10000\\\\n\");\n",
        "      printf(\"label total number = 10000\\\\n\");\n",
        "    }\n",
        "\n",
        "    // 2-4. check the number of rows and columns\n",
        "    // Check the number of rows and columns\n",
        "    char num_rows[4];\n",
        "    char num_cols[4];\n",
        "    fread(&num_rows, sizeof(char), 4, image_file);\n",
        "    fread(&num_cols, sizeof(char), 4, image_file);\n",
        "    if (mnist_bin_to_int(num_rows) != INSIZE || mnist_bin_to_int(num_cols) != INSIZE) {\n",
        "        printf(\"Invalid image size\\\\n\");\n",
        "        fclose(image_file);\n",
        "        fclose(label_file);\n",
        "        return -1;\n",
        "    }else{\n",
        "      printf(\"rows = 28, cols = 28\\\\n\");\n",
        "    }\n",
        "\n",
        "    // Allocate memory for the data set\n",
        "    mnist_data *data = (mnist_data*)malloc(NUM_TEST_IMAGES * sizeof(mnist_data));\n",
        "\n",
        "    int counter = 0;\n",
        "\n",
        "    // 3. loads image data as double type (from 0.0 to 1.0 dividing unsigned char values by 255.0) \n",
        "    unsigned char image[INSIZE][INSIZE];\n",
        "    for (int i = 0; i < NUM_TEST_IMAGES; i++) {\n",
        "        fread(image, sizeof(image), 1, image_file);\n",
        "        data[i].label = fgetc(label_file);\n",
        "        counter++;\n",
        "        for (int j = 0; j < INSIZE; j++) {\n",
        "            for (int k = 0; k < INSIZE; k++) {\n",
        "                data[i].data[j][k] = (double)image[j][k] / 255.0;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // 4. closes opened files\n",
        "    fclose(image_file);\n",
        "    fclose(label_file);\n",
        "\n",
        "    // Set the output variables\n",
        "    *data_set = data;\n",
        "    *count = counter;\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "    // CUDA kernel functions for filtering, bias, and sigmoid activation\n",
        "\n",
        "    __global__ void kernel_conv_filter(float (*input)[28], float (*pre_output)[24][24], float (*weight)[5][5]) {\n",
        "            int t = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "            int i = t / (24 * 24);\n",
        "            int j = (t / 24) % 24;\n",
        "            int k = t % 24;\n",
        "\n",
        "            if (t < 6 * 24 * 24) {\n",
        "                float sum = 0.0f;\n",
        "                for (int m = 0; m < 5; m++) {\n",
        "                    for (int n = 0; n < 5; n++) {\n",
        "                        sum += weight[i][m][n] * input[j + m][k + n];\n",
        "                    }\n",
        "                }\n",
        "                pre_output[i][j][k] = sum;\n",
        "            }\n",
        "    }\n",
        "\n",
        "\n",
        "    __global__ void kernel_conv_bias(float (*pre_output)[24][24], float *bias) {\n",
        "        int t = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "        int i = t / (24 * 24);\n",
        "        int j = (t / 24) % 24;\n",
        "        int k = t % 24;\n",
        "\n",
        "        if (t < 6 * 24 * 24) {\n",
        "            pre_output[i][j][k] += bias[i];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    __global__ void kernel_conv_sigmoid(float pre_output[6][24][24], float output[6][24][24]) {\n",
        "        int t = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "        int i = t / (24 * 24);\n",
        "        int j = (t / 24) % 24;\n",
        "        int k = t % 24;\n",
        "        if (t < 6 * 24 * 24) {\n",
        "           output[i][j][k] = 1.0f / (1.0f + expf(-pre_output[i][j][k]));\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "    __global__ void kernel_ss1_filter(float input[6][24][24], float output[6][6][6], float weight[4][4]) {\n",
        "        int row = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "        int col = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "        int feature = blockIdx.z;\n",
        "        \n",
        "        if (row < 6 && col < 6) {\n",
        "            float sum = 0.0;\n",
        "            for (int i = 0; i < 4; i++) {\n",
        "                for (int j = 0; j < 4; j++) {\n",
        "                    sum += input[feature][row*4+i][col*4+j] * weight[i][j];\n",
        "                }\n",
        "            }\n",
        "            output[feature][row][col] = sum;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    __global__ void kernel_ss1_bias(float output[6][6][6]) {\n",
        "        int row = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "        int col = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "        int feature = blockIdx.z;\n",
        "        if (row < 6 && col < 6) {\n",
        "            output[feature][row][col] = output[feature][row][col] + 0.827946;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    __global__ void kernel_ss1_sigmoid(float output[6][6][6]) {\n",
        "        int row = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "        int col = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "        int feature = blockIdx.z;\n",
        "        if (row < 6 && col < 6) {\n",
        "          output[feature][row][col] = 1.0f / (1.0f + expf(-output[feature][row][col]));\n",
        "        }\n",
        "    }\n",
        "\n",
        "__global__ void kernel_fc1(float input[6][6][6], float pre_output[10], float weight[10][6][6][6]) {\n",
        "    // Calculate global thread index\n",
        "    int t = threadIdx.x + blockIdx.x * blockDim.x ;\n",
        "    if(t<10){\n",
        "          // Compute dot product of input and weight for current output index\n",
        "    float dot_product = 0.0f;\n",
        "    for (int x = 0; x < 6; x++) {\n",
        "        for (int y = 0; y < 6; y++) {\n",
        "            for (int z = 0; z < 6; z++) {\n",
        "                dot_product += input[x][y][z] * weight[t][x][y][z];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    // Store dot product in pre_output array\n",
        "    pre_output[t] = dot_product;\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "__global__ void kernel_fc1_bias(float pre_output[10], float bias[10]) {\n",
        "    int t = threadIdx.x +  blockIdx.x * blockDim.x;\n",
        "    if (t < 10) {\n",
        "        pre_output[t] += bias[t];\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void kernel_fc1_sigmoid(float pre_output[10], float output[10]) {\n",
        "    int t = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    if (t < 10) {\n",
        "        output[t] = 1.0f / (1.0f + expf(-pre_output[t]));\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "class Layer {\n",
        "public:\n",
        "    int M, N, O;\n",
        "    float pre_output[6][24][24], output[6][24][24];\n",
        "    float weight[6][5][5], bias[24];\n",
        "\n",
        "    float ssweight[4][4];\n",
        "    float ssoutput[6][6][6];\n",
        "\n",
        "    float (*dpre_output)[24][24], (*doutput)[24][24];\n",
        "    float (*dweight)[5][5], (*dbias);\n",
        "    float (*dssweight)[4], (*dssoutput)[6][6];\n",
        "\n",
        "    float fc_output[10]; \n",
        "    float fc_weight[10][6][6][6]; \n",
        "    float fc_bias[10];\n",
        "\n",
        "    float (*dfc_pre_output);\n",
        "    float (*dfc_output);\n",
        "    float (*dfc_weight)[6][6][6]; \n",
        "    float (*dfc_bias);\n",
        "\n",
        "\n",
        "\n",
        "    Layer(int M, int N, int O) {\n",
        "        this->M = M;\n",
        "        this->N = N;\n",
        "        this->O = O;\n",
        "/*\n",
        "        this->weight = c1_weight;\n",
        "        this->bias = c1_bias;\n",
        "\n",
        "        this->ssweight = s2_weight;\n",
        "\n",
        "        this->fc_weight = f3_weight;\n",
        "        this->fc_bias = f3_bias;\n",
        "*/\n",
        "        memcpy(weight, c1_weight, sizeof(c1_weight));\n",
        "        memcpy(bias, c1_bias, sizeof(c1_bias));\n",
        "        memcpy(ssweight, s2_weight, sizeof(s2_weight));\n",
        "        memcpy(fc_weight, f3_weight, sizeof(f3_weight));\n",
        "        memcpy(fc_bias, f3_bias, sizeof(f3_bias));\n",
        "\n",
        "\n",
        "        // Allocate memory on the GPU\n",
        "        cudaMalloc(&dpre_output, 6 * O * O * sizeof(float));\n",
        "        cudaMalloc(&doutput, 6 * O * O * sizeof(float));\n",
        "        cudaMalloc(&dweight, M * 5 * 5 * sizeof(float));\n",
        "        cudaMalloc(&dbias, O * sizeof(float));\n",
        "        cudaMalloc(&dssweight, 4 * 4 *sizeof(float));\n",
        "        cudaMalloc(&dssoutput, 6 * 6 * 6 * sizeof(float));\n",
        "\n",
        "        cudaMalloc(&dfc_pre_output, 10 * sizeof(float));\n",
        "        cudaMalloc(&dfc_output, 10 * sizeof(float));\n",
        "        cudaMalloc(&dfc_weight, 10 * 6 * 6 * 6 * sizeof(float));\n",
        "        cudaMalloc(&dfc_bias, 10 * sizeof(float));\n",
        "  }\n",
        "    ~Layer() {\n",
        "        // Free memory on the GPU\n",
        "        cudaFree(dpre_output);\n",
        "        cudaFree(doutput);\n",
        "        cudaFree(dweight);\n",
        "        cudaFree(dbias);\n",
        "    }\n",
        "};\n",
        "\n",
        "static double forward_pass(double data[28][28],Layer layer) {\n",
        "        float input[28][28];\n",
        "        // Convert the input data to -1 to simulate test\n",
        "        for (int i = 0; i < 28; i++) {\n",
        "            for (int j = 0; j < 28; j++) {\n",
        "                input[i][j] = data[i][j];\n",
        "            }\n",
        "        }\n",
        "        \n",
        "\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    cudaEventRecord(start); // cudaEventRecord(start, 0); over 10.x\n",
        "\n",
        "        //CONVOLUTION\n",
        "\n",
        "        float (*dinput)[28];\n",
        "        cudaMalloc(&dinput, 28 * 28 * sizeof(float));\n",
        "        cudaMemcpy(dinput, input, 28 * 28 * sizeof(float), cudaMemcpyHostToDevice);\n",
        "        cudaMemcpy(layer.dweight, layer.weight, 6 * 5 * 5 * sizeof(float), cudaMemcpyHostToDevice);\n",
        "        cudaMemcpy(layer.dbias, layer.bias, 6 * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "        cudaMemcpy(layer.dssweight, layer.ssweight, 4 * 4 * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "        // Perform the filtering operation\n",
        "        kernel_conv_filter<<<6, 576>>>(dinput, layer.dpre_output, layer.dweight);\n",
        "        \n",
        "        // Add the bias term\n",
        "        kernel_conv_bias<<<6, 576>>>(layer.dpre_output, layer.dbias);\n",
        "\n",
        "        // Apply the sigmoid activation function\n",
        "        kernel_conv_sigmoid<<<6, 576>>>(layer.dpre_output, layer.doutput);\n",
        "\n",
        "\n",
        "        //SUBSAMPLING\n",
        "\n",
        "        dim3 blockDim(16, 16);\n",
        "        dim3 gridDim(2, 2, 6);\n",
        "\n",
        "        kernel_ss1_filter<<<gridDim, blockDim>>>(layer.doutput, layer.dssoutput, layer.dssweight);\n",
        "\n",
        "        kernel_ss1_bias<<<gridDim, blockDim>>>(layer.dssoutput);\n",
        "\n",
        "        kernel_ss1_sigmoid<<<gridDim, blockDim>>>(layer.dssoutput);\n",
        "\n",
        "\n",
        "        // Call kernel_fc1 function\n",
        "        cudaMemcpy(layer.dfc_weight, layer.fc_weight, 10 * 6 * 6 * 6 * sizeof(float), cudaMemcpyHostToDevice);\n",
        "        cudaMemcpy(layer.dfc_bias, layer.fc_bias, 10 * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "\n",
        "        kernel_fc1<<<1, 10>>>(layer.dssoutput, layer.dfc_pre_output, layer.dfc_weight);\n",
        "    \n",
        "        kernel_fc1_bias<<<1, 10>>>(layer.dfc_pre_output, layer.dfc_bias);\n",
        "      \n",
        "        kernel_fc1_sigmoid<<<1, 10>>>(layer.dfc_pre_output, layer.dfc_output);\n",
        "\n",
        "        cudaMemcpy(layer.fc_output, layer.dfc_output, 10 * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "        \n",
        "        \n",
        "        /*\n",
        "        for (int k = 0; k < 10; k++) {\n",
        "              printf(\"%f  .  \", layer.fc_output[k]);\n",
        "\n",
        "              printf(\"%d  .  \\\\n\", k);\n",
        "        }\n",
        "        */\n",
        "        cudaEventRecord(stop); // cudaEventRecord(stop, 0); over 10.x\n",
        "        cudaEventSynchronize(stop);\n",
        "        float elapsedTime;\n",
        "        cudaEventElapsedTime(&elapsedTime, start, stop);\n",
        "\n",
        "        cudaEventDestroy(start);\n",
        "        cudaEventDestroy(stop);\n",
        "\n",
        "        return (double)elapsedTime; \n",
        "              \n",
        "}   \n",
        "\n",
        "\n",
        "double time_taken = 0.0;\n",
        "\n",
        "int main() {\n",
        "    const int M = 6;\n",
        "    const int N = 1;\n",
        "    const int O = 24;\n",
        "\n",
        "    int ret, i; \n",
        "    mnist_data *test_set; \n",
        "    static unsigned int test_cnt;\n",
        "\n",
        "    // Initialize convolutional layer\n",
        "    Layer layer(M, N, O);\n",
        "    \n",
        "\n",
        "    // 1. load data\n",
        "    if(ret = mnist_load(\"data/t10k-images.idx3-ubyte\", \"data/t10k-labels.idx1-ubyte\", &test_set, &test_cnt) != 0){\n",
        "        printf(\"An error occured: %d \\\\n\", ret);\n",
        "    } else {\n",
        "        printf(\"test_cnt = %d \\\\n\", test_cnt); // test_cnt must have the number of test images (i.e., 10K) // copy the trained parameters to GPU device (in here or another layer)\n",
        "    }\n",
        "    \n",
        "\n",
        "    // forward pass\n",
        "    unsigned int error = 0; \n",
        "    unsigned int max = 0; \n",
        "    float res[10];\n",
        "    for (i=0; i<test_cnt; i++){\n",
        "        time_taken += forward_pass(test_set[i].data, layer);\n",
        "        cudaMemcpy(res, layer.dfc_output, sizeof(float)*10, cudaMemcpyDeviceToHost); \n",
        "        for(int j=0; j<10; j++){\n",
        "            if (res[max] < res[j]){\n",
        "                max = j; \n",
        "            }\n",
        "        }    \n",
        "        if (max != test_set[i].label){\n",
        "            ++error;\n",
        "        } \n",
        "    }\n",
        "    printf(\"Error Rate = %f%% (%d out of 10,000)\\\\n\", double(error)/double(test_cnt)*100.0, error); \n",
        "    printf(\"Accuracy = %.3f%% (%d out of 10,000)\\\\n\", 100.0 - double(error)/double(test_cnt)*100.0, test_cnt - error);\n",
        "    printf(\"Ex time = %f (ms) \\\\n\", time_taken); //NOTE: cudaMemcpy operations also should be added\n",
        "\n",
        "\n",
        "\n",
        "    return 0;\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "/*START_CITE\n",
        "https://chat.openai.com/chat\n",
        "END_CITE*/\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "gQws4P-30kXn"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = open(\"mnist_data_test.cu\", \"w\")\n",
        "text_file.write(code1)\n",
        "text_file.close()"
      ],
      "metadata": {
        "id": "z85vYaZe2CRD"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc mnist_data_test.cu -o mnist_data_test"
      ],
      "metadata": {
        "id": "rPrvQs3U2C0u"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./mnist_data_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9kQUIqC2DDn",
        "outputId": "8a34bc45-3240-43a1-ea30-a71082a8a5f7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "image magic number= 2051\n",
            "label magic number = 2049\n",
            "image total number = 10000\n",
            "label total number = 10000\n",
            "rows = 28, cols = 28\n",
            "test_cnt = 10000 \n",
            "Error Rate = 6.060000% (606 out of 10,000)\n",
            "Accuracy = 93.940% (9394 out of 10,000)\n",
            "Ex time = 832.637055 (ms) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected Output:\n",
        "\n",
        "test_cnt = 10000\n",
        "\n",
        "Error Rate = 5.880% (588 out of 10,000)\n",
        "\n",
        "Accuracy = 94.120% (9412 out of 10,000)\n",
        "\n",
        "\n",
        "Unfortunately there are 18 digits that are predicted wrongly from expected output\n",
        "\n",
        "Overall code works for mnist data recognition , however there are still room for optimization and cleaning code.\n",
        "\n",
        "I could not submit it yesterday because google colab did not connected to gpu, due to absence of subscription and it was busy."
      ],
      "metadata": {
        "id": "m1v-v_31eS54"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I4bbIp5Ef9OV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}